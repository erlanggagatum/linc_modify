{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/gatum/Projects/Neurosymbolic-AI/linc2/linc_modify/notebooks', '/home/gatum/.conda/envs/linc_modify/lib/python310.zip', '/home/gatum/.conda/envs/linc_modify/lib/python3.10', '/home/gatum/.conda/envs/linc_modify/lib/python3.10/lib-dynload', '', '/home/gatum/.conda/envs/linc_modify/lib/python3.10/site-packages']\n",
      "[2024-04-19 10:19:40,021] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# # sys.path.append(\"../eval/\")\n",
    "# sys.path.append(\"../eval/task\")\n",
    "os.chdir('..')\n",
    "# sys.path = sys.path[:6]\n",
    "# sys.path.append(\"/../\")\n",
    "print(sys.path)\n",
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import openai\n",
    "import transformers\n",
    "import fnmatch\n",
    "import datasets\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from functools import cache\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from warnings import warn\n",
    "from abc import abstractmethod, ABC\n",
    "from eval import tasks\n",
    "from eval.tasks import utils\n",
    "from eval.utils import TokenizedDataset, complete_code\n",
    "from eval.tasks.utils import evaluate, convert_to_nltk_rep\n",
    "# from eval.generation import parallel_generations\n",
    "from diskcache import Cache\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser\n",
    "\n",
    "from eval import tasks\n",
    "from eval.generation import parallel_generations\n",
    "from eval.args import RunnerArguments, HFArguments, OAIArguments, GenerationArguments\n",
    "# from eval.evaluator import HFEvaluator, OAIEvaluator\n",
    "from eval.tasks import ALL_TASKS, TASK_REGISTRY\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('minimario/FOLIO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fewshot_indices_all = [\n",
    "    125,\n",
    "    23,\n",
    "    60,\n",
    "    275,\n",
    "    148,\n",
    "    261,\n",
    "    263,\n",
    "    683,\n",
    "    299,\n",
    "    684,\n",
    "    850,\n",
    "    853,\n",
    "    886,\n",
    "    892,\n",
    "    930,\n",
    "    980,\n",
    "]\n",
    "\n",
    "\n",
    "shot = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a first-order logic (FOL) problem.\n",
      "The problem is to determine whether the conclusion follows from the premises.\n",
      "The premises are given in the form of a set of first-order logic sentences.\n",
      "The conclusion is given in the form of a single first-order logic sentence.\n",
      "The task is to translate each of the premises and conclusions into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\n",
      "Expressions should be adhere to the format of the Python NLTK package logic module.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_instructions(mode = 'neurosymbolic'):\n",
    "    instructions = \"\"\n",
    "    instructions += \"The following is a first-order logic (FOL) problem.\\n\"\n",
    "    instructions += \"The problem is to determine whether the conclusion follows from the premises.\\n\"\n",
    "    instructions += \"The premises are given in the form of a set of first-order logic sentences.\\n\"\n",
    "    instructions += \"The conclusion is given in the form of a single first-order logic sentence.\\n\"\n",
    "    if mode == \"baseline\":\n",
    "        instructions += f\"The task is to evaluate the conclusion as 'True', 'False', or 'Uncertain' given the premises.\"\n",
    "    else:\n",
    "        instructions += \"The task is to translate each of the premises and conclusions into FOL expressions, \"\n",
    "        if mode == \"scratchpad\":\n",
    "            instructions += f\"and then to evaluate the conclusion as 'True', 'False', or 'Uncertain' given the premises.\"\n",
    "        elif mode == \"neurosymbolic\":\n",
    "            instructions += \"so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\\n\"\n",
    "            instructions += \"Expressions should be adhere to the format of the Python NLTK package logic module.\"\n",
    "    return instructions + \"\\n\\n\"\n",
    "\n",
    "print(get_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_train_example(doc, mode='neurosymbolic'):\n",
    "    example = format_test_example(doc)\n",
    "    if mode == \"baseline\":\n",
    "        example += f\"{doc['label'].strip()}\\n\"\n",
    "    elif mode == \"cot\":\n",
    "        example += f\"{doc['cot']}\\n\"\n",
    "    else:\n",
    "        for premise, fol in zip(doc[\"premises\"], doc[\"premises-FOL\"]):\n",
    "            example += f\"TEXT:\\t{premise.strip()}\\nFOL:\\t{fol.strip()}\\n\"\n",
    "        example += f\"TEXT:\\t{doc['conclusion'].strip()}\\nFOL:\\t{doc['conclusion-FOL'].strip()}\\n\"\n",
    "        if mode == \"scratchpad\":\n",
    "            example += f\"ANSWER:\\t{doc['label'].strip()}\\n\"\n",
    "    return example + \"</EVALUATE>\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_test_example(doc):\n",
    "    example = \"<PREMISES>\\n\"\n",
    "    for premise in doc[\"premises\"]:\n",
    "        example += f\"{premise.strip()}\\n\"\n",
    "    example += \"</PREMISES>\\n\"\n",
    "    example += f\"<CONCLUSION>\\n{doc['conclusion'].strip()}\\n</CONCLUSION>\\n\"\n",
    "    example += \"<EVALUATE>\\n\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_fol_samples_train(train_dataset):\n",
    "        def reformat_fol_sample(sample):\n",
    "            sample[\"premises-FOL\"] = [\n",
    "                convert_to_nltk_rep(premise) for premise in sample[\"premises-FOL\"]\n",
    "            ]\n",
    "            return sample\n",
    "\n",
    "        return train_dataset.map(reformat_fol_sample)\n",
    "\n",
    "def add_conclusion_fols_train(train_dataset):\n",
    "    train_conclusion_fols = {\n",
    "        23: \"HigherRank(RealMadrid, Barcelona)\",\n",
    "        60: \"-OlympicGoldMedalWinner(Amy) -> NobelLaureate(Amy)\",\n",
    "        125: \"-Dispensable(Worksheet)\",\n",
    "        148: \"FolkSong(Inception)\",\n",
    "        261: \"MakeGoodBreakfast(Luke)\",\n",
    "        263: \"exists x. (Develops(Ets, x) & For(x, k-OneTwoandhighereducation)) & exists x. (Develops(Ets, x) & AssociatedWith(x, Entrytouseducationinstitutions))\",\n",
    "        275: \"ContributeToCountry(James)\",\n",
    "        299: \"GetRhythmRight(John)\",\n",
    "        683: \"exists x. (BRICS(x) & Speaks(x, Hindi))\",\n",
    "        684: \"Film(Hamilton)\",\n",
    "        850: \"-Liked(Leo, Charlie) & -Cares(Charlie, Leo)\",\n",
    "        853: \"Won(Threebodyproblem, Hugoaward)\",\n",
    "        886: \"Dagfinn(DagfinnAarskog)\",\n",
    "        892: \"PartOf(Minsk, Scottishpremiership)\",\n",
    "        930: \"-Locate(Boves, Europe)\",\n",
    "        980: \"(InvitedTakePhoto(James) & -HappyCommunicate(James)) | (-InvitedTakePhoto(James) & HappyCommunicate(James))\",\n",
    "    }\n",
    "    conclusions = [None for _ in range(len(train_dataset))]\n",
    "    for index, conclusion_fol in train_conclusion_fols.items():\n",
    "        conclusions[index] = conclusion_fol\n",
    "    train_dataset = train_dataset.add_column(\"conclusion-FOL\", conclusions)\n",
    "    return train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_examples():\n",
    "    \"\"\"\n",
    "    Returns a few-shot example for the task.\n",
    "    :param n: int\n",
    "        number of examples\n",
    "    :param seed: int\n",
    "        seed for random number generator\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "    \n",
    "    _nshot = shot\n",
    "    examples = []\n",
    "    for doc in train.select(range(_nshot)):\n",
    "        examples.append(format_train_example(doc))\n",
    "    return \"\\n\".join(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(doc):\n",
    "    \"\"\"\n",
    "    Builds the prompt for the LM to generate from.\n",
    "    :param doc: dict[str: str]\n",
    "        sample from the test dataset\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "    instructions = get_instructions()\n",
    "    train = fewshot_examples()\n",
    "    test = format_test_example(doc)\n",
    "    prompt = \"\\n\".join([instructions, train, test])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a first-order logic (FOL) problem.\n",
      "The problem is to determine whether the conclusion follows from the premises.\n",
      "The premises are given in the form of a set of first-order logic sentences.\n",
      "The conclusion is given in the form of a single first-order logic sentence.\n",
      "The task is to translate each of the premises and conclusions into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\n",
      "Expressions should be adhere to the format of the Python NLTK package logic module.\n",
      "\n",
      "\n",
      "<PREMISES>\n",
      "All dispensable things are environment-friendly.\n",
      "All woodware is dispensable.\n",
      "All paper is woodware.\n",
      "No good things are bad.\n",
      "All environment-friendly things are good.\n",
      "A worksheet is either paper or is environment-friendly.\n",
      "</PREMISES>\n",
      "<CONCLUSION>\n",
      "A worksheet is not dispensable.\n",
      "</CONCLUSION>\n",
      "<EVALUATE>\n",
      "TEXT:\tAll dispensable things are environment-friendly.\n",
      "FOL:\tall x. (Dispensable(x) -> EnvironmentFriendly(x))\n",
      "TEXT:\tAll woodware is dispensable.\n",
      "FOL:\tall x. (Woodware(x) -> Dispensable(x))\n",
      "TEXT:\tAll paper is woodware.\n",
      "FOL:\tall x. (Paper(x) -> Woodware(x))\n",
      "TEXT:\tNo good things are bad.\n",
      "FOL:\tall x. (Good(x) -> -Bad(x))\n",
      "TEXT:\tAll environment-friendly things are good.\n",
      "FOL:\tall x. (EnvironmentFriendly(x) -> Good(x))\n",
      "TEXT:\tA worksheet is either paper or is environment-friendly.\n",
      "FOL:\t((Paper(Worksheet) & -EnvironmentFriendly(Worksheet)) | (-Paper(Worksheet) & EnvironmentFriendly(Worksheet)))\n",
      "TEXT:\tA worksheet is not dispensable.\n",
      "FOL:\t-Dispensable(Worksheet)\n",
      "</EVALUATE>\n",
      "\n",
      "<PREMISES>\n",
      "A La Liga soccer team ranks higher than another if it receives more points.\n",
      "If two La Liga soccer teams recieve the same points, the team which recieves more points from the games between the two teams ranks higher.\n",
      "Real Madrid and Barcelona are both La Liga soccer teams.\n",
      "In La Liga 2021-2022, Real Madrid recieves 86 points and Barcelon recieves 73 points.\n",
      "In La Liga 2021-2022, Real Madrid and Barcelona both recieve 3 points from the games between them.\n",
      "</PREMISES>\n",
      "<CONCLUSION>\n",
      "In La Liga 2021-2022, Real Madrid ranks higher than Barcelona.\n",
      "</CONCLUSION>\n",
      "<EVALUATE>\n",
      "TEXT:\tA La Liga soccer team ranks higher than another if it receives more points.\n",
      "FOL:\tall x. all y. (LaLiga(x) & LaLiga(y) & MorePoints(x, y) -> HigherRank(x, y))\n",
      "TEXT:\tIf two La Liga soccer teams recieve the same points, the team which recieves more points from the games between the two teams ranks higher.\n",
      "FOL:\tall x. all y. (LaLiga(x) & LaLiga(y) & -MorePoints(x, y) & -MorePoints(y, x) & MorePointsInGameBetween(x, y) -> HigherRank(x, y))\n",
      "TEXT:\tReal Madrid and Barcelona are both La Liga soccer teams.\n",
      "FOL:\tLaLiga(RealMadrid) & LaLiga(Barcelona)\n",
      "TEXT:\tIn La Liga 2021-2022, Real Madrid recieves 86 points and Barcelon recieves 73 points.\n",
      "FOL:\tMorePoints(RealMadrid, Barcelona)\n",
      "TEXT:\tIn La Liga 2021-2022, Real Madrid and Barcelona both recieve 3 points from the games between them.\n",
      "FOL:\t-MorePointsInGameBetween(RealMadrid, Barcelona) & -MorePointsInGameBetween(Barcelona, RealMadrid)\n",
      "TEXT:\tIn La Liga 2021-2022, Real Madrid ranks higher than Barcelona.\n",
      "FOL:\tHigherRank(RealMadrid, Barcelona)\n",
      "</EVALUATE>\n",
      "\n",
      "<PREMISES>\n",
      "All athletes are good at sports.\n",
      "All Olympic gold medal winners are good athletes.\n",
      "No scientists are good at sports.\n",
      "All Nobel laureates are scientists.\n",
      "Amy is good at sports or Amy is an Olympic gold medal winner.\n",
      "If Amy is not a Nobel laureate, then Amy is not an Olympic gold medal winner.\n",
      "</PREMISES>\n",
      "<CONCLUSION>\n",
      "If Amy is not an Olympic gold medal winner, then Amy is a Nobel laureate.\n",
      "</CONCLUSION>\n",
      "<EVALUATE>\n",
      "TEXT:\tAll athletes are good at sports.\n",
      "FOL:\tall x. (Athlete(x) -> GoodAtSports(x))\n",
      "TEXT:\tAll Olympic gold medal winners are good athletes.\n",
      "FOL:\tall x. (OlympicGoldMedalWinner(x) -> Athlete(x))\n",
      "TEXT:\tNo scientists are good at sports.\n",
      "FOL:\tall x. (Scientist(x) -> -GoodAtSports(x))\n",
      "TEXT:\tAll Nobel laureates are scientists.\n",
      "FOL:\tall x. (NobelLaureate(x) -> Scientist(x))\n",
      "TEXT:\tAmy is good at sports or Amy is an Olympic gold medal winner.\n",
      "FOL:\tGoodAtSports(Amy) | OlympicGoldMedalWinner(Amy)\n",
      "TEXT:\tIf Amy is not a Nobel laureate, then Amy is not an Olympic gold medal winner.\n",
      "FOL:\t-NobelLaureate(Amy) -> -OlympicGoldMedalWinner(Amy)\n",
      "TEXT:\tIf Amy is not an Olympic gold medal winner, then Amy is a Nobel laureate.\n",
      "FOL:\t-OlympicGoldMedalWinner(Amy) -> NobelLaureate(Amy)\n",
      "</EVALUATE>\n",
      "\n",
      "<PREMISES>\n",
      "All people who regularly drink coffee are dependent on caffeine.\n",
      "People either regularly drink coffee or joke about being addicted to caffeine.\n",
      "No one who jokes about being addicted to caffeine is unaware that caffeine is a drug.\n",
      "Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug.\n",
      "If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
      "</PREMISES>\n",
      "<CONCLUSION>\n",
      "Rina is a person who jokes about being addicted to caffeine or unaware that caffeine is a drug.\n",
      "</CONCLUSION>\n",
      "<EVALUATE>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INIT\n",
    "train_dataset = dataset['train']\n",
    "_train_dataset = reformat_fol_samples_train(train_dataset)\n",
    "_train_dataset = add_conclusion_fols_train(_train_dataset)\n",
    "_train_dataset = _train_dataset.map(\n",
    "    lambda x: {\"label\": \"Uncertain\" if x[\"label\"] == \"Unknown\" else x[\"label\"]},\n",
    "    remove_columns=[\"label\"],\n",
    ")\n",
    "\n",
    "train_fewshot_indices = train_fewshot_indices_all[:shot]\n",
    "train = _train_dataset.select(train_fewshot_indices)\n",
    "\n",
    "print(get_prompt(dataset['train'][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
