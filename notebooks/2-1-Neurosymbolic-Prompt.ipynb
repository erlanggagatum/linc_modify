{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/gatum/Projects/Neurosymbolic-AI/linc2/linc_modify/notebooks', '/home/gatum/.conda/envs/linc_modify/lib/python310.zip', '/home/gatum/.conda/envs/linc_modify/lib/python3.10', '/home/gatum/.conda/envs/linc_modify/lib/python3.10/lib-dynload', '', '/home/gatum/.conda/envs/linc_modify/lib/python3.10/site-packages']\n",
      "[2024-04-19 10:19:40,021] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# # sys.path.append(\"../eval/\")\n",
    "# sys.path.append(\"../eval/task\")\n",
    "os.chdir('..')\n",
    "# sys.path = sys.path[:6]\n",
    "# sys.path.append(\"/../\")\n",
    "print(sys.path)\n",
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import openai\n",
    "import transformers\n",
    "import fnmatch\n",
    "import datasets\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from functools import cache\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from warnings import warn\n",
    "from abc import abstractmethod, ABC\n",
    "from eval import tasks\n",
    "from eval.tasks import utils\n",
    "from eval.utils import TokenizedDataset, complete_code\n",
    "from eval.tasks.utils import evaluate, convert_to_nltk_rep\n",
    "# from eval.generation import parallel_generations\n",
    "from diskcache import Cache\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser\n",
    "\n",
    "from eval import tasks\n",
    "from eval.generation import parallel_generations\n",
    "from eval.args import RunnerArguments, HFArguments, OAIArguments, GenerationArguments\n",
    "# from eval.evaluator import HFEvaluator, OAIEvaluator\n",
    "from eval.tasks import ALL_TASKS, TASK_REGISTRY\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('minimario/FOLIO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fewshot_indices_all = [\n",
    "    125,\n",
    "    23,\n",
    "    60,\n",
    "    275,\n",
    "    148,\n",
    "    261,\n",
    "    263,\n",
    "    683,\n",
    "    299,\n",
    "    684,\n",
    "    850,\n",
    "    853,\n",
    "    886,\n",
    "    892,\n",
    "    930,\n",
    "    980,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a first-order logic (FOL) problem.\n",
      "The problem is to determine whether the conclusion follows from the premises.\n",
      "The premises are given in the form of a set of first-order logic sentences.\n",
      "The conclusion is given in the form of a single first-order logic sentence.\n",
      "The task is to translate each of the premises and conclusions into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\n",
      "Expressions should be adhere to the format of the Python NLTK package logic module.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_instructions(mode = 'neurosymbolic'):\n",
    "    instructions = \"\"\n",
    "    instructions += \"The following is a first-order logic (FOL) problem.\\n\"\n",
    "    instructions += \"The problem is to determine whether the conclusion follows from the premises.\\n\"\n",
    "    instructions += \"The premises are given in the form of a set of first-order logic sentences.\\n\"\n",
    "    instructions += \"The conclusion is given in the form of a single first-order logic sentence.\\n\"\n",
    "    if mode == \"baseline\":\n",
    "        instructions += f\"The task is to evaluate the conclusion as 'True', 'False', or 'Uncertain' given the premises.\"\n",
    "    else:\n",
    "        instructions += \"The task is to translate each of the premises and conclusions into FOL expressions, \"\n",
    "        if mode == \"scratchpad\":\n",
    "            instructions += f\"and then to evaluate the conclusion as 'True', 'False', or 'Uncertain' given the premises.\"\n",
    "        elif mode == \"neurosymbolic\":\n",
    "            instructions += \"so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\\n\"\n",
    "            instructions += \"Expressions should be adhere to the format of the Python NLTK package logic module.\"\n",
    "    return instructions + \"\\n\\n\"\n",
    "\n",
    "print(get_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_train_example(doc, mode='neurosymbolic'):\n",
    "    example = format_test_example(doc)\n",
    "    if mode == \"baseline\":\n",
    "        example += f\"{doc['label'].strip()}\\n\"\n",
    "    elif mode == \"cot\":\n",
    "        example += f\"{doc['cot']}\\n\"\n",
    "    else:\n",
    "        for premise, fol in zip(doc[\"premises\"], doc[\"premises-FOL\"]):\n",
    "            example += f\"TEXT:\\t{premise.strip()}\\nFOL:\\t{fol.strip()}\\n\"\n",
    "        example += f\"TEXT:\\t{doc['conclusion'].strip()}\\nFOL:\\t{doc['conclusion-FOL'].strip()}\\n\"\n",
    "        if mode == \"scratchpad\":\n",
    "            example += f\"ANSWER:\\t{doc['label'].strip()}\\n\"\n",
    "    return example + \"</EVALUATE>\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_test_example(doc):\n",
    "    example = \"<PREMISES>\\n\"\n",
    "    for premise in doc[\"premises\"]:\n",
    "        example += f\"{premise.strip()}\\n\"\n",
    "    example += \"</PREMISES>\\n\"\n",
    "    example += f\"<CONCLUSION>\\n{doc['conclusion'].strip()}\\n</CONCLUSION>\\n\"\n",
    "    example += \"<EVALUATE>\\n\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_fol_samples_train(train_dataset):\n",
    "        def reformat_fol_sample(sample):\n",
    "            sample[\"premises-FOL\"] = [\n",
    "                convert_to_nltk_rep(premise) for premise in sample[\"premises-FOL\"]\n",
    "            ]\n",
    "            return sample\n",
    "\n",
    "        return train_dataset.map(reformat_fol_sample)\n",
    "\n",
    "def add_conclusion_fols_train(train_dataset):\n",
    "    train_conclusion_fols = {\n",
    "        23: \"HigherRank(RealMadrid, Barcelona)\",\n",
    "        60: \"-OlympicGoldMedalWinner(Amy) -> NobelLaureate(Amy)\",\n",
    "        125: \"-Dispensable(Worksheet)\",\n",
    "        148: \"FolkSong(Inception)\",\n",
    "        261: \"MakeGoodBreakfast(Luke)\",\n",
    "        263: \"exists x. (Develops(Ets, x) & For(x, k-OneTwoandhighereducation)) & exists x. (Develops(Ets, x) & AssociatedWith(x, Entrytouseducationinstitutions))\",\n",
    "        275: \"ContributeToCountry(James)\",\n",
    "        299: \"GetRhythmRight(John)\",\n",
    "        683: \"exists x. (BRICS(x) & Speaks(x, Hindi))\",\n",
    "        684: \"Film(Hamilton)\",\n",
    "        850: \"-Liked(Leo, Charlie) & -Cares(Charlie, Leo)\",\n",
    "        853: \"Won(Threebodyproblem, Hugoaward)\",\n",
    "        886: \"Dagfinn(DagfinnAarskog)\",\n",
    "        892: \"PartOf(Minsk, Scottishpremiership)\",\n",
    "        930: \"-Locate(Boves, Europe)\",\n",
    "        980: \"(InvitedTakePhoto(James) & -HappyCommunicate(James)) | (-InvitedTakePhoto(James) & HappyCommunicate(James))\",\n",
    "    }\n",
    "    conclusions = [None for _ in range(len(train_dataset))]\n",
    "    for index, conclusion_fol in train_conclusion_fols.items():\n",
    "        conclusions[index] = conclusion_fol\n",
    "    train_dataset = train_dataset.add_column(\"conclusion-FOL\", conclusions)\n",
    "    return train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_examples():\n",
    "    \"\"\"\n",
    "    Returns a few-shot example for the task.\n",
    "    :param n: int\n",
    "        number of examples\n",
    "    :param seed: int\n",
    "        seed for random number generator\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "    \n",
    "    _nshot = 1\n",
    "    examples = []\n",
    "    for doc in train.select(range(_nshot)):\n",
    "        examples.append(format_train_example(doc))\n",
    "    return \"\\n\".join(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conclusion', 'premises', 'premises-FOL', 'label', 'conclusion-FOL'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INIT\n",
    "train_dataset = dataset['train']\n",
    "_train_dataset = reformat_fol_samples_train(train_dataset)\n",
    "_train_dataset = add_conclusion_fols_train(_train_dataset)\n",
    "_train_dataset = _train_dataset.map(\n",
    "    lambda x: {\"label\": \"Uncertain\" if x[\"label\"] == \"Unknown\" else x[\"label\"]},\n",
    "    remove_columns=[\"label\"],\n",
    ")\n",
    "\n",
    "shot = 1\n",
    "train_fewshot_indices = train_fewshot_indices_all[:shot]\n",
    "train = _train_dataset.select(train_fewshot_indices)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a first-order logic (FOL) problem.\n",
      "The problem is to determine whether the conclusion follows from the premises.\n",
      "The premises are given in the form of a set of first-order logic sentences.\n",
      "The conclusion is given in the form of a single first-order logic sentence.\n",
      "The task is to translate each of the premises and conclusions into FOL expressions, so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\n",
      "Expressions should be adhere to the format of the Python NLTK package logic module.\n",
      "\n",
      "\n",
      "<PREMISES>\n",
      "All dispensable things are environment-friendly.\n",
      "All woodware is dispensable.\n",
      "All paper is woodware.\n",
      "No good things are bad.\n",
      "All environment-friendly things are good.\n",
      "A worksheet is either paper or is environment-friendly.\n",
      "</PREMISES>\n",
      "<CONCLUSION>\n",
      "A worksheet is not dispensable.\n",
      "</CONCLUSION>\n",
      "<EVALUATE>\n",
      "TEXT:\tAll dispensable things are environment-friendly.\n",
      "FOL:\tall x. (Dispensable(x) -> EnvironmentFriendly(x))\n",
      "TEXT:\tAll woodware is dispensable.\n",
      "FOL:\tall x. (Woodware(x) -> Dispensable(x))\n",
      "TEXT:\tAll paper is woodware.\n",
      "FOL:\tall x. (Paper(x) -> Woodware(x))\n",
      "TEXT:\tNo good things are bad.\n",
      "FOL:\tall x. (Good(x) -> -Bad(x))\n",
      "TEXT:\tAll environment-friendly things are good.\n",
      "FOL:\tall x. (EnvironmentFriendly(x) -> Good(x))\n",
      "TEXT:\tA worksheet is either paper or is environment-friendly.\n",
      "FOL:\t((Paper(Worksheet) & -EnvironmentFriendly(Worksheet)) | (-Paper(Worksheet) & EnvironmentFriendly(Worksheet)))\n",
      "TEXT:\tA worksheet is not dispensable.\n",
      "FOL:\t-Dispensable(Worksheet)\n",
      "</EVALUATE>\n",
      "\n",
      "<PREMISES>\n",
      "All people who regularly drink coffee are dependent on caffeine.\n",
      "People either regularly drink coffee or joke about being addicted to caffeine.\n",
      "No one who jokes about being addicted to caffeine is unaware that caffeine is a drug.\n",
      "Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug.\n",
      "If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
      "</PREMISES>\n",
      "<CONCLUSION>\n",
      "Rina is a person who jokes about being addicted to caffeine or unaware that caffeine is a drug.\n",
      "</CONCLUSION>\n",
      "<EVALUATE>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_prompt(doc):\n",
    "    \"\"\"\n",
    "    Builds the prompt for the LM to generate from.\n",
    "    :param doc: dict[str: str]\n",
    "        sample from the test dataset\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "    instructions = get_instructions()\n",
    "    train = fewshot_examples()\n",
    "    test = format_test_example(doc)\n",
    "    prompt = \"\\n\".join([instructions, train, test])\n",
    "    return prompt\n",
    "\n",
    "print(get_prompt(dataset['train'][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
