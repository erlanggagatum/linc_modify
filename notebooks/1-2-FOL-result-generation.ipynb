{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../eval/')\n",
    "\n",
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import openai\n",
    "import transformers\n",
    "import fnmatch\n",
    "import datasets\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from functools import cache\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from warnings import warn\n",
    "from abc import abstractmethod, ABC\n",
    "from eval import tasks\n",
    "from tasks import utils\n",
    "from eval.utils import TokenizedDataset, complete_code\n",
    "from eval.tasks.utils import evaluate, convert_to_nltk_rep\n",
    "# from eval.generation import parallel_generations\n",
    "from diskcache import Cache\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser\n",
    "\n",
    "from eval import tasks\n",
    "from eval.generation import parallel_generations\n",
    "from eval.args import RunnerArguments, HFArguments, OAIArguments, GenerationArguments\n",
    "# from eval.evaluator import HFEvaluator, OAIEvaluator\n",
    "from eval.tasks import ALL_TASKS, TASK_REGISTRY\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file \n",
    "# LLM generated \n",
    "f = open('../output/bigcode/starcoderplus_folio-neurosymbolic-1shot_generations_raw.json')\n",
    "data = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Error', 'Error', 'Error', 'True', 'True', 'Uncertain', 'Error', 'Uncertain', 'Error', 'Uncertain', 'Uncertain', 'Error', 'Uncertain', 'True', 'Error', 'Error', 'True', 'Error', 'Uncertain', 'Error', 'Error', 'Error', 'Error', 'Error', 'Error', 'Error', 'Error', 'Error', 'False', 'Error', 'True', 'True', 'Error', 'Error', 'Error', 'Error', 'Uncertain', 'Uncertain', 'True', 'Error', 'Error', 'Uncertain', 'Uncertain', 'Error', 'True', 'Uncertain', 'Error', 'Uncertain', 'Error', 'Uncertain', 'Uncertain', 'Error', 'Uncertain', 'Error', 'Error', 'Error', 'Error', 'Uncertain', 'Uncertain', 'True', 'Uncertain', 'True', 'Uncertain', 'Uncertain', 'Error', 'Uncertain', 'Error', 'False', 'True', 'Error', 'Error', 'False', 'Uncertain', 'Error', 'False', 'Uncertain', 'Error', 'True', 'Error', 'Error', 'Error', 'Uncertain', 'Error', 'Uncertain', 'False', 'Uncertain', 'Error', 'Uncertain', 'Error', 'Error', 'Error', 'True', 'Error', 'Error', 'Uncertain', 'Error', 'Uncertain', 'Uncertain', 'Error', 'Error', 'Error', 'Error', 'Error', 'Error', 'Uncertain', 'Uncertain', 'Error', 'False', 'Error', 'True', 'Error', 'Uncertain', 'Error', 'Error', 'Error', 'Error', 'True', 'True', 'Error', 'Error', 'Error', 'Error', 'True', 'Error', 'Uncertain', 'False', 'Uncertain', 'Error', 'True', 'Error', 'Error', 'False', 'True', 'Error', 'Error', 'Error', 'Error', 'Error', 'Error', 'Uncertain', 'Uncertain', 'Error', 'Error', 'Uncertain', 'Error', 'Error', 'Uncertain', 'Error', 'Uncertain', 'Uncertain', 'Error', 'False', 'Error', 'Error', 'Error', 'Uncertain', 'Error', 'Error', 'Error', 'Error', 'Uncertain', 'False', 'Error', 'Uncertain', 'Error', 'False', 'Error', 'Error', 'Error', 'Error', 'Error', 'Error', 'True', 'Error', 'Uncertain', 'True', 'True', 'True', 'Error', 'Error', 'Error', 'Error']\n"
     ]
    }
   ],
   "source": [
    "# Data sample\n",
    "index = 5\n",
    "sample = data[index][0]\n",
    "results = []\n",
    "\n",
    "for sample in data:\n",
    "    sample_split = sample[0].split('\\n')\n",
    "    sample_split\n",
    "    length_range = range(0, len(sample_split))\n",
    "    evaluate_content = []\n",
    "\n",
    "    eval_end = False\n",
    "    eval_start = False\n",
    "\n",
    "    # Extract <EVALUATE> tag\n",
    "    for i in reversed(length_range):\n",
    "        if (eval_end == True and eval_start == True):\n",
    "            break\n",
    "        elif (sample_split[i] == '</EVALUATE>'):\n",
    "            eval_end = True\n",
    "        elif (sample_split[i] == '<EVALUATE>'):\n",
    "            eval_start = True\n",
    "        evaluate_content.append(sample_split[i])\n",
    "\n",
    "    # Extract FOL only\n",
    "    fol = []\n",
    "    for line in list(reversed(evaluate_content)):\n",
    "        if \"FOL\" in line:\n",
    "            try:\n",
    "                fol.append(line.split('\\t')[1])\n",
    "            except:\n",
    "                # print(line)\n",
    "                continue\n",
    "            # print(line.split('\\t')[1])\n",
    "    fol\n",
    "\n",
    "    # FOL Prover\n",
    "    premises = fol[:-1]\n",
    "    conclusion = fol[-1]\n",
    "\n",
    "    # print(premises, conclusion)\n",
    "    try:\n",
    "        # print(utils.evaluate(premises, conclusion))\n",
    "        results.append(utils.evaluate(premises, conclusion))\n",
    "    except:\n",
    "        results.append('Error')\n",
    "        # print('Error')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) == len(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
